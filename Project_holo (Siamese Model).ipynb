{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40ee8b2-cec4-4517-a223-f83a3020ff48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All HOLO graphs have been saved in the directory: sh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import os\n",
    "\n",
    "def load_and_process_data(prefix, folder):\n",
    "    node_file = f'{folder}/{prefix}_nodes.csv'\n",
    "    link_file = f'{folder}/{prefix}_links.csv'\n",
    "\n",
    "    node_data = pd.read_csv(node_file)\n",
    "    link_data = pd.read_csv(link_file)\n",
    "\n",
    "    # Assuming the ground truth labels are in the same format\n",
    "    labels = node_data['ground_truth'].values\n",
    "    features = node_data.drop(columns=['ground_truth'])\n",
    "\n",
    "    node_features = features[['atom_type', 'residue_type', 'radius', 'voromqa_sas_potential', 'residue_mean_sas_potential', 'residue_sum_sas_potential', 'residue_size', 'sas_area', 'voromqa_sas_energy', 'voromqa_depth', 'voromqa_score_a', 'voromqa_score_r', 'volume', 'volume_vdw', 'ufsr_a1', 'ufsr_a2', 'ufsr_c2', 'ufsr_c3', 'ev28', 'ev56']]\n",
    "    link_features = link_data[['atom_index1', 'atom_index2','area', 'boundary', 'distance', 'voromqa_energy', 'seq_sep_class', 'covalent_bond', 'hbond']]\n",
    "\n",
    "    edge_index = torch.tensor(np.array([link_features['atom_index1'].values, link_features['atom_index2'].values]), dtype=torch.long)\n",
    "\n",
    "    self_links = torch.arange(0, len(node_features))\n",
    "    edge_index = torch.cat([edge_index, torch.stack([self_links, self_links])], dim=1)\n",
    "    edge_index = torch.cat([edge_index, edge_index[[1, 0], :]], dim=1)  # Add reverse direction\n",
    "\n",
    "    node_features_tensor = torch.tensor(node_features.values, dtype=torch.float)\n",
    "    labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=node_features_tensor, edge_index=edge_index, y=labels_tensor)\n",
    "\n",
    "    return data\n",
    "\n",
    "candidate_pairs_file = 'holo/candidate_pairs.txt'\n",
    "candidate_pairs = pd.read_csv(candidate_pairs_file, delim_whitespace=True)\n",
    "\n",
    "graphs = {}\n",
    "for index, row in candidate_pairs.iterrows():\n",
    "    holo_prefix = f\"{row['holo_pdb_id']}_{row['holo_chain_id']}\"\n",
    "    graphs[holo_prefix] = load_and_process_data(holo_prefix, 'holo')\n",
    "\n",
    "save_dir = 'sh'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for prefix, graph in graphs.items():\n",
    "    save_path = os.path.join(save_dir, f'{prefix}_graph.pt')\n",
    "    torch.save(graph, save_path)\n",
    "\n",
    "print(f\"All HOLO graphs have been saved in the directory: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e52dac72-36db-4d0f-875e-aaf18c1840d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flahaari/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APO Batch:\n",
      "x shape: torch.Size([75498, 20])\n",
      "edge_index shape: torch.Size([2, 1227630])\n",
      "batch shape: torch.Size([75498])\n",
      "y shape: torch.Size([75498])\n",
      "\n",
      "HOLO Batch:\n",
      "x shape: torch.Size([75486, 20])\n",
      "edge_index shape: torch.Size([2, 1230080])\n",
      "batch shape: torch.Size([75486])\n",
      "y shape: torch.Size([75486])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import DataLoader, Batch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to load data pairs\n",
    "def load_data_pairs(apo_folder='sg', holo_folder='sh', pairs_file='apo/candidate_pairs.txt'):\n",
    "    pairs = pd.read_csv(pairs_file, delim_whitespace=True)\n",
    "    data_pairs = []\n",
    "    for index, row in pairs.iterrows():\n",
    "        apo_prefix = f\"{row['apo_pdb_id']}_{row['apo_chain_id']}\"\n",
    "        holo_prefix = f\"{row['holo_pdb_id']}_{row['holo_chain_id']}\"\n",
    "        apo_graph_path = os.path.join(apo_folder, f'{apo_prefix}_graph.pt')\n",
    "        holo_graph_path = os.path.join(holo_folder, f'{holo_prefix}_graph.pt')\n",
    "        if os.path.exists(apo_graph_path) and os.path.exists(holo_graph_path):\n",
    "            apo_graph = torch.load(apo_graph_path)\n",
    "            holo_graph = torch.load(holo_graph_path)\n",
    "            data_pairs.append((apo_graph, holo_graph))\n",
    "    return data_pairs\n",
    "\n",
    "# Function to collate graphs for DataLoader\n",
    "def collate_graphs(data_list):\n",
    "    apo_data_list, holo_data_list = zip(*data_list)\n",
    "    apo_batch = Batch.from_data_list(apo_data_list)\n",
    "    holo_batch = Batch.from_data_list(holo_data_list)\n",
    "    return apo_batch, holo_batch\n",
    "\n",
    "# Load data pairs and set up DataLoader\n",
    "data_pairs = load_data_pairs()\n",
    "train_loader = DataLoader(data_pairs, batch_size=32, shuffle=True, collate_fn=collate_graphs)\n",
    "\n",
    "# Inspect the first batch from DataLoader\n",
    "first_batch = next(iter(train_loader))\n",
    "apo_batch, holo_batch = first_batch\n",
    "\n",
    "# Print the shapes of the tensors in the first batch\n",
    "print(\"APO Batch:\")\n",
    "print(\"x shape:\", apo_batch.x.shape)\n",
    "print(\"edge_index shape:\", apo_batch.edge_index.shape)\n",
    "print(\"batch shape:\", apo_batch.batch.shape)\n",
    "print(\"y shape:\", apo_batch.y.shape)\n",
    "\n",
    "print(\"\\nHOLO Batch:\")\n",
    "print(\"x shape:\", holo_batch.x.shape)\n",
    "print(\"edge_index shape:\", holo_batch.edge_index.shape)\n",
    "print(\"batch shape:\", holo_batch.batch.shape)\n",
    "print(\"y shape:\", holo_batch.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ccde12-290b-4439-bb74-83532d104193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeda619d-2c3c-40e4-8581-c0577581eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/flahaari/.local/lib/python3.8/site-packages/torch_geometric/deprecation.py:22: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n",
      "Epochs:   0%|                                            | 0/10 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [70727] to be smaller than self [32] apart from dimension 0 and to be smaller size than src [1]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 101\u001b[0m\n\u001b[1;32m     98\u001b[0m loss_history \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m     loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 74\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Pass both apo and holo data to the model\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapo_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholo_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Assuming the target labels are in apo_batch.y\u001b[39;00m\n\u001b[1;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39msqueeze(), apo_batch\u001b[38;5;241m.\u001b[39my)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m, in \u001b[0;36mSiameseGVPModel.forward\u001b[0;34m(self, apo_data, holo_data)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, apo_data, holo_data):\n\u001b[0;32m---> 20\u001b[0m     apo_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapo_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     holo_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_structure(holo_data)\n\u001b[1;32m     22\u001b[0m     combined \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([apo_x, holo_x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m, in \u001b[0;36mSiameseGVPModel.process_structure\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgvp2(x)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mgeom_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_mean_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply pooling only if batch tensor is present\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Default to mean pooling if batch tensor is not present\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/nn/pool/glob.py:61\u001b[0m, in \u001b[0;36mglobal_mean_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch_geometric/utils/scatter.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n\u001b[0;32m---> 74\u001b[0m     \u001b[43mcount\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_ones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     77\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [70727] to be smaller than self [32] apart from dimension 0 and to be smaller size than src [1]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader, Batch\n",
    "import torch_geometric.nn as geom_nn\n",
    "import gvp\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the Siamese GVP Model\n",
    "class SiameseGVPModel(nn.Module):\n",
    "    def __init__(self, node_in_dims):\n",
    "        super(SiameseGVPModel, self).__init__()\n",
    "        self.gvp1 = gvp.GVP(node_in_dims, (64, 0), vector_gate=True, activations=(F.relu, None))\n",
    "        self.gvp2 = gvp.GVP((64, 0), (1, 0))\n",
    "\n",
    "    def forward(self, apo_data, holo_data):\n",
    "        apo_x = self.process_structure(apo_data)\n",
    "        holo_x = self.process_structure(holo_data)\n",
    "        combined = torch.cat([apo_x, holo_x], dim=1)\n",
    "        combined = F.relu(self.fc1(combined))\n",
    "        combined = self.fc2(combined)\n",
    "        return combined\n",
    "\n",
    "    def process_structure(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.gvp1(x)\n",
    "        x = self.gvp2(x)\n",
    "        if hasattr(data, 'batch') and data.batch is not None:\n",
    "            x = geom_nn.global_mean_pool(x[0], data.batch)  # Apply pooling only if batch tensor is present\n",
    "        else:\n",
    "            x = x.mean(dim=0, keepdim=True)  # Default to mean pooling if batch tensor is not present\n",
    "        return x\n",
    "\n",
    "def collate_graphs(data_list):\n",
    "    apo_data_list, holo_data_list = zip(*data_list)\n",
    "    apo_batch = Batch.from_data_list(apo_data_list)\n",
    "    holo_batch = Batch.from_data_list(holo_data_list)\n",
    "    return apo_batch, holo_batch\n",
    "\n",
    "# Load paired data\n",
    "def load_data_pairs(apo_folder='sg', holo_folder='sh', pairs_file='apo/candidate_pairs.txt'):\n",
    "    pairs = pd.read_csv(pairs_file, delim_whitespace=True)\n",
    "    data_pairs = []\n",
    "\n",
    "    for index, row in pairs.iterrows():\n",
    "        apo_prefix = f\"{row['apo_pdb_id']}_{row['apo_chain_id']}\"\n",
    "        holo_prefix = f\"{row['holo_pdb_id']}_{row['holo_chain_id']}\"\n",
    "\n",
    "        apo_graph_path = os.path.join(apo_folder, f'{apo_prefix}_graph.pt')\n",
    "        holo_graph_path = os.path.join(holo_folder, f'{holo_prefix}_graph.pt')\n",
    "\n",
    "        if os.path.exists(apo_graph_path) and os.path.exists(holo_graph_path):\n",
    "            apo_graph = torch.load(apo_graph_path)\n",
    "            holo_graph = torch.load(holo_graph_path)\n",
    "            data_pairs.append((apo_graph, holo_graph))\n",
    "\n",
    "    return data_pairs\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_data in loader:\n",
    "        # Unpack the batch data\n",
    "        apo_batch, holo_batch = batch_data\n",
    "        apo_batch = apo_batch.to(device)\n",
    "        holo_batch = holo_batch.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass both apo and holo data to the model\n",
    "        output = model(apo_batch, holo_batch)\n",
    "\n",
    "        # Assuming the target labels are in apo_batch.y\n",
    "        loss = criterion(output.squeeze(), apo_batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Load data pairs\n",
    "data_pairs = load_data_pairs()\n",
    "\n",
    "# Set up DataLoader\n",
    "train_loader = DataLoader(data_pairs, batch_size=32, shuffle=True, collate_def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for apo_batch, holo_batch in loader:\n",
    "        apo_batch, holo_batch = apo_batch.to(device), holo_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        apo_output = model(apo_batch)\n",
    "        holo_output = model(holo_batch)\n",
    "        output = torch.cat([apo_output, holo_output], dim=1)  # Concatenate the outputs\n",
    "        # Assume that the ground truth labels are the same for both apo and holo data\n",
    "        loss = criterion(output, apo_batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)fn=collate_graphs)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseGVPModel(node_in_dims=(20, 0)).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    loss_history.append(loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5e18f8e-e279-4c7f-abab-d47350f2123f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_data_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(loader)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Load data pairs and setup DataLoader\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m data_pairs \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_pairs\u001b[49m()\n\u001b[1;32m     45\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(data_pairs, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_graphs)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Model, optimizer, and loss function\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_data_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.data import DataLoader, Batch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SiameseGCNModel(nn.Module):\n",
    "    def __init__(self, node_in_dims, node_out_dims=64):\n",
    "        super(SiameseGCNModel, self).__init__()\n",
    "        self.gcn1 = GCNConv(node_in_dims, node_out_dims)\n",
    "        self.gcn2 = GCNConv(node_out_dims, node_out_dims)\n",
    "\n",
    "    def forward(self, apo_data, holo_data):\n",
    "        apo_x = self.process_structure(apo_data)\n",
    "        holo_x = self.process_structure(holo_data)\n",
    "        combined = torch.cat([apo_x, holo_x], dim=1)\n",
    "        return combined\n",
    "\n",
    "    def process_structure(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = F.relu(self.gcn1(x, edge_index))\n",
    "        x = self.gcn2(x, edge_index)\n",
    "        x = global_mean_pool(x, data.batch)  # Apply global mean pooling\n",
    "        return x\n",
    "\n",
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for apo_batch, holo_batch in loader:\n",
    "        apo_batch = apo_batch.to(device)\n",
    "        holo_batch = holo_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(apo_batch, holo_batch)\n",
    "        loss = criterion(output, apo_batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Load data pairs and setup DataLoader\n",
    "data_pairs = load_data_pairs()\n",
    "train_loader = DataLoader(data_pairs, batch_size=32, shuffle=True, collate_fn=collate_graphs)\n",
    "\n",
    "# Model, optimizer, and loss function\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SiameseGCNModel(node_in_dims=20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "loss_history = []\n",
    "\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "    loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    loss_history.append(loss)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "# Plotting the training loss\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_history, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c540f3-209c-4601-9ef2-50ab68cdee20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
